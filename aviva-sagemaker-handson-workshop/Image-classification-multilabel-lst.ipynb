{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification multi-label classification\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prequisites)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "4. [Training](#Training)\n",
    "5. [Hyperparameter Tuning](#Launch-hyperparameter-tuning-job)\n",
    "6. [Inference](#Inference)\n",
    "7. [Clean-up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our end-to-end example of image classification using the Sagemaker 1P image classification algorithm. In this demo, we will use the Amazon sagemaker image classification algorithm in transfer learning mode to fine-tune a pre-trained model (trained on imagenet data) to learn to classify a new dataset. In particular, the pre-trained model will be fine-tuned using [Car damage dataset](https://peltarion.com/knowledge-center/tutorials/classifying-car-damage). \n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. There are three parts to this:\n",
    "\n",
    "* The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "* The S3 bucket that you want to use for training and model data\n",
    "* The Amazon sagemaker image classification docker image which need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::130159455024:role/service-role/AmazonSageMaker-ExecutionRole-20190428T145852\n",
      "using bucket sagemaker-us-east-1-130159455024\n",
      "CPU times: user 308 ms, sys: 9.19 ms, total: 317 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'image_classification'\n",
    "\n",
    "print('using bucket %s'%bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this notebook, we will use the car damage dataset for classification.The dataset has around 1500 images of 8 different categories.These are: \n",
    "\n",
    "1. Broken headlamp\n",
    "2. Broken tail lamp\n",
    "3. Glass shatter\n",
    "4. Door scratch\n",
    "5. Door dent\n",
    "6. Bumper dent\n",
    "7. Bumper scratch\n",
    "8. Unknown\n",
    "\n",
    "We then split this dataset into a train and holdout dataset for fine tuning the model and testing our final accuracy\n",
    "\n",
    "The image classification algorithm can take two types of input formats. The first is a [recordio format](https://mxnet.incubator.apache.org/tutorials/basic/record_io.html) and the other is a [lst format](https://mxnet.incubator.apache.org/how_to/recordio.html?highlight=im2rec). We will use the lst file format for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 20.1b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip -q install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the annotation to create lst file\n",
    "Use pycocotools to parse the annotation and create the lst file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image/0.jpeg</td>\n",
       "      <td>unknown</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image/1.jpeg</td>\n",
       "      <td>head_lamp</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image/2.jpeg</td>\n",
       "      <td>door_scratch</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image/3.jpeg</td>\n",
       "      <td>head_lamp</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image/4.jpeg</td>\n",
       "      <td>unknown</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image         class subset\n",
       "0  image/0.jpeg       unknown      T\n",
       "1  image/1.jpeg     head_lamp      T\n",
       "2  image/2.jpeg  door_scratch      T\n",
       "3  image/3.jpeg     head_lamp      T\n",
       "4  image/4.jpeg       unknown      T"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('index.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown', 'head_lamp', 'door_scratch', 'glass_shatter', 'tail_lamp', 'bumper_dent', 'door_dent', 'bumper_scratch']\n"
     ]
    }
   ],
   "source": [
    "cats = list(data['class'].unique())\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes = data[data['subset']=='T']\n",
    "test_indexes = data[data['subset']=='V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes = train_indexes.drop(['subset'], axis=1)\n",
    "test_indexes = test_indexes.drop(['subset'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating the number of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indexes.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating train.lst file using index.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.lst', 'w') as fp:\n",
    "    for ind in train_indexes.index: \n",
    "        string =''\n",
    "        string += str(ind)\n",
    "        string += '\\t'\n",
    "        class_index = cats.index(str(train_indexes['class'][ind]))\n",
    "        for i in range(num_classes):\n",
    "            if(class_index != i ):\n",
    "                string += str(0)\n",
    "            else:\n",
    "                string += str(1)\n",
    "            string += '\\t'\n",
    "        string += str(train_indexes['image'][ind]).split('/')[-1]\n",
    "        \n",
    "        fp.write(string)\n",
    "        fp.write('\\n')\n",
    "    fp.close()\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating test.lst file using index.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.lst', 'w') as fp:\n",
    "    for ind in test_indexes.index: \n",
    "        string =''\n",
    "        string += str(ind)\n",
    "        string += '\\t'\n",
    "        class_index = cats.index(str(test_indexes['class'][ind]))\n",
    "        for i in range(num_classes):\n",
    "            if(class_index != i ):\n",
    "                string += str(0)\n",
    "            else:\n",
    "                string += str(1)\n",
    "            string += '\\t'\n",
    "        string += str(test_indexes['image'][ind]).split('/')[-1]\n",
    "        \n",
    "        fp.write(string)\n",
    "        fp.write('\\n')\n",
    "    fp.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "Upload the data onto the s3 bucket. The images are uploaded onto train and validation bucket. The lst files are uploaded to train_lst and validation_lst folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "s3train_lst = 's3://{}/{}/train_lst/'.format(bucket, prefix)\n",
    "s3validation_lst = 's3://{}/{}/validation_lst/'.format(bucket, prefix)\n",
    "\n",
    "# upload the image files to train and validation channels\n",
    "!aws s3 cp image $s3train --recursive --quiet\n",
    "!aws s3 cp image $s3validation --recursive --quiet\n",
    "\n",
    "# upload the lst files to train_lst and validation_lst channels\n",
    "!aws s3 cp train.lst $s3train_lst --quiet\n",
    "!aws s3 cp test.lst $s3validation_lst --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our image classifier. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job.\n",
    "\n",
    "### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "* **Training instance count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings. \n",
    "* **Training instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training \n",
    "* **Output path**: This the s3 folder in which the training output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm parameters\n",
    "\n",
    "Apart from the above set of parameters, there are hyperparameters that are specific to the algorithm. These are:\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. We use 18 in this samples but other values such as 50, 152 can be used.\n",
    "* **use_pretrained_model**: Set to 1 to use pretrained model for transfer learning.\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the dataset. We use 5 classes from MSCOCO and hence it is set to 5\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run\n",
    "* **resize**: Resize the image before using it for training. The images are resized so that the shortest side is of this parameter. If the parameter is not set, then the training data is used as such without resizing.\n",
    "* **epochs**: Number of training epochs\n",
    "* **learning_rate**: Learning rate for training\n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 2500 for COCO dataset with the current split\n",
    "* **use_weighted_loss**: This parameter is used to balance the influence of the positive and negative samples within the dataset.\n",
    "* **augmentation_type**: This parameter determines the type of augmentation used for training. It can take on three values, 'crop', 'crop_color' and 'crop_color_transform'\n",
    "* **precision_dtype**: The data type precision used during training. Using ``float16`` can lead to faster training with minimal drop in accuracy, paritcularly on P3 machines. By default, the parameter is set to ``float32``\n",
    "* **multi_label**: Set multi_label to 1 for multi-label processing\n",
    "\n",
    "You can find a detailed description of all the algorithm parameters at https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_hyperparameters(num_layers=18,\n",
    "                             use_pretrained_model=1,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=8,\n",
    "                             mini_batch_size=128,\n",
    "                             resize=256,\n",
    "                             epochs=5,\n",
    "                             learning_rate=0.001,\n",
    "                             num_training_samples=train_indexes.shape[0],\n",
    "                             use_weighted_loss=1,\n",
    "                             augmentation_type = 'crop_color_transform',\n",
    "                             precision_dtype='float32',\n",
    "                             multi_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data specification\n",
    "Set the data type and channels used for training. In this training, we use application/x-image content type that require individual images and lst file for data input. In addition, Sagemaker image classification algorithm supports application/x-recordio format which can be used for larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated', \n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "train_data_lst = sagemaker.session.s3_input(s3train_lst, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated', \n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n",
    "                 'validation_lst': validation_data_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "Start training by calling the fit method in the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-27 11:06:26 Starting - Starting the training job...\n",
      "2020-04-27 11:06:27 Starting - Launching requested ML instances......\n",
      "2020-04-27 11:07:33 Starting - Preparing the instances for training......\n",
      "2020-04-27 11:08:48 Downloading - Downloading input data......\n",
      "2020-04-27 11:09:53 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:57 INFO 140253248313152] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:57 INFO 140253248313152] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'image_shape': u'3,224,224', u'use_pretrained_model': u'1', u'num_layers': u'18', u'epochs': u'5', u'resize': u'256', u'augmentation_type': u'crop_color_transform', u'multi_label': u'0', u'precision_dtype': u'float32', u'mini_batch_size': u'128', u'use_weighted_loss': u'1', u'num_classes': u'8', u'num_training_samples': u'1275'}\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:57 INFO 140253248313152] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.001', u'multi_label': u'0', u'epochs': u'5', u'use_weighted_loss': u'1', u'lr_scheduler_factor': 0.1, u'num_layers': u'18', u'num_classes': u'8', u'precision_dtype': u'float32', u'mini_batch_size': u'128', u'resize': u'256', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'augmentation_type': u'crop_color_transform', u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'1275'}\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:57 WARNING 140253248313152] use_weighted_loss is only used for multi-label training. Ignoring the parameter.\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:57 INFO 140253248313152] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:57 INFO 140253248313152] Creating record files for train.lst\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:59 INFO 140253248313152] Done creating record files...\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:59 INFO 140253248313152] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:09:59 INFO 140253248313152] Creating record files for test.lst\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] Done creating record files...\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] multi_label: 0\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] num_layers: 18\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] epochs: 5\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] image resize size: 256\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] learning_rate: 0.001\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] num_training_samples: 1275\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] mini_batch_size: 128\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] num_classes: 8\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] augmentation_type: crop_color_transform\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] kv_store: device\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] --------------------\u001b[0m\n",
      "\u001b[34m[11:10:00] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[11:10:00] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:00 INFO 140253248313152] Setting number of threads: 7\u001b[0m\n",
      "\u001b[34m[11:10:10] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.2633.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:12 INFO 140253248313152] Epoch[0] Train-accuracy=0.332465\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:12 INFO 140253248313152] Epoch[0] Time cost=2.067\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:12 INFO 140253248313152] Epoch[0] Validation-accuracy=0.820312\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:12 INFO 140253248313152] Storing the best model with validation accuracy: 0.820312\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:13 INFO 140253248313152] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:14 INFO 140253248313152] Epoch[1] Train-accuracy=0.836806\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:14 INFO 140253248313152] Epoch[1] Time cost=1.289\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:14 INFO 140253248313152] Epoch[1] Validation-accuracy=0.893229\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:14 INFO 140253248313152] Storing the best model with validation accuracy: 0.893229\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:14 INFO 140253248313152] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\n",
      "2020-04-27 11:10:24 Uploading - Uploading generated training model\u001b[34m[04/27/2020 11:10:16 INFO 140253248313152] Epoch[2] Train-accuracy=0.907118\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:16 INFO 140253248313152] Epoch[2] Time cost=1.408\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:16 INFO 140253248313152] Epoch[2] Validation-accuracy=0.886719\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:18 INFO 140253248313152] Epoch[3] Train-accuracy=0.914931\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:18 INFO 140253248313152] Epoch[3] Time cost=1.343\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:18 INFO 140253248313152] Epoch[3] Validation-accuracy=0.906250\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:18 INFO 140253248313152] Storing the best model with validation accuracy: 0.906250\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:18 INFO 140253248313152] Saved checkpoint to \"/opt/ml/model/image-classification-0004.params\"\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:20 INFO 140253248313152] Epoch[4] Train-accuracy=0.932292\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:20 INFO 140253248313152] Epoch[4] Time cost=1.319\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:20 INFO 140253248313152] Epoch[4] Validation-accuracy=0.910156\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:20 INFO 140253248313152] Storing the best model with validation accuracy: 0.910156\u001b[0m\n",
      "\u001b[34m[04/27/2020 11:10:20 INFO 140253248313152] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\n",
      "2020-04-27 11:10:36 Completed - Training job completed\n",
      "Training seconds: 108\n",
      "Billable seconds: 108\n",
      "CPU times: user 536 ms, sys: 27.1 ms, total: 563 ms\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the tuning job with the following configurations need to be specified:\n",
    "\n",
    "1. the hyperparameters that SageMaker Automatic Model Tuning will tune: learning_rate, mini_batch_size and optimizer\n",
    "2. the maximum number of training jobs it will run to optimize the objective metric: 20\n",
    "3. the number of parallel training jobs that will run in the tuning job: 2\n",
    "4. the objective metric that Automatic Model Tuning will use: validation:accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch hyperparameter tuning job \n",
    "Now we can launch a hyperparameter tuning job by calling fit in tuner,setting early_stopping_type='Auto' to enable automatic training job early stopping. We will wait until the tuning finished, which may take around 2 hours ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime \n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "tuning_job_name = 'image-classification1'\n",
    "\n",
    "hyperparameter_ranges = {'learning_rate': ContinuousParameter(0.00001, 1.0),\n",
    "                         'mini_batch_size': IntegerParameter(16, 64),\n",
    "                         'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'nag']),\n",
    "                         'mini_batch_size': IntegerParameter(64,128)}\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(model, \n",
    "                            objective_metric_name, \n",
    "                            hyperparameter_ranges,\n",
    "                            objective_type='Maximize', \n",
    "                            max_jobs=20, \n",
    "                            max_parallel_jobs=2,\n",
    "                            early_stopping_type='Auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n",
      "CPU times: user 2.85 s, sys: 81.9 ms, total: 2.93 s\n",
      "Wall time: 45min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tuner.fit(inputs=data_channels, job_name=tuning_job_name, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning finished, the top 5 performing hyperparameters can be listed below. One can analyse the results deeper by using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mini_batch_size</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.931759</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2020-04-27 11:41:20+00:00</td>\n",
       "      <td>image-classification1-011-99979921</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-04-27 11:39:43+00:00</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>127.0</td>\n",
       "      <td>nag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.925325</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2020-04-27 11:45:51+00:00</td>\n",
       "      <td>image-classification1-014-bec1fc44</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-04-27 11:44:10+00:00</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>77.0</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.923372</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2020-04-27 11:36:55+00:00</td>\n",
       "      <td>image-classification1-010-9f96cc41</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-04-27 11:35:08+00:00</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>87.0</td>\n",
       "      <td>nag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2020-04-27 11:41:26+00:00</td>\n",
       "      <td>image-classification1-012-f72a40c4</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-04-27 11:39:46+00:00</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>91.0</td>\n",
       "      <td>rmsprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.908136</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2020-04-27 11:50:12+00:00</td>\n",
       "      <td>image-classification1-016-0043c0fb</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-04-27 11:48:36+00:00</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>127.0</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FinalObjectiveValue  TrainingElapsedTimeSeconds           TrainingEndTime  \\\n",
       "9   0.931759             97.0                       2020-04-27 11:41:20+00:00   \n",
       "6   0.925325             101.0                      2020-04-27 11:45:51+00:00   \n",
       "10  0.923372             107.0                      2020-04-27 11:36:55+00:00   \n",
       "8   0.923077             100.0                      2020-04-27 11:41:26+00:00   \n",
       "4   0.908136             96.0                       2020-04-27 11:50:12+00:00   \n",
       "\n",
       "                       TrainingJobName TrainingJobStatus  \\\n",
       "9   image-classification1-011-99979921  Completed          \n",
       "6   image-classification1-014-bec1fc44  Completed          \n",
       "10  image-classification1-010-9f96cc41  Completed          \n",
       "8   image-classification1-012-f72a40c4  Completed          \n",
       "4   image-classification1-016-0043c0fb  Completed          \n",
       "\n",
       "           TrainingStartTime  learning_rate  mini_batch_size optimizer  \n",
       "9  2020-04-27 11:39:43+00:00  0.022877       127.0            nag       \n",
       "6  2020-04-27 11:44:10+00:00  0.000039       77.0             rmsprop   \n",
       "10 2020-04-27 11:35:08+00:00  0.002682       87.0             nag       \n",
       "8  2020-04-27 11:39:46+00:00  0.000158       91.0             rmsprop   \n",
       "4  2020-04-27 11:48:36+00:00  0.002452       127.0            sgd       "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total training time and training jobs status can be checked with the following script. Because automatic early stopping is by default off, all the training jobs should be completed normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training time is 0.56 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Completed    18\n",
       "Stopped      2 \n",
       "Name: TrainingJobStatus, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sage_client = boto3.Session().client('sagemaker')\n",
    "\n",
    "#tuning_job_name = 'image-classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track hyperparameter tuning job progress\n",
    "After you launch a tuning job, you can see its progress by calling describe_tuning_job API. The output from describe-tuning-job is a JSON object that contains information about the current state of the tuning job. You can call list_training_jobs_for_tuning_job to see a detailed list of the training jobs that the tuning job launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 training jobs have completed\n"
     ]
    }
   ],
   "source": [
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "    \n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch all results as DataFrame\n",
    "We can list hyperparameters and objective metrics of all training jobs and pick up the training job with the best objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training jobs with valid objective: 20\n",
      "{'lowest': 0.11009199917316437, 'highest': 0.9317589998245239}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tuner = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "full_df = tuner.dataframe()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df['FinalObjectiveValue'] > -float('inf')]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values('FinalObjectiveValue', ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\":min(df['FinalObjectiveValue']),\"highest\": max(df['FinalObjectiveValue'])})\n",
    "        pd.set_option('display.max_colwidth', -1)  # Don't truncate TrainingJobName        \n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See TuningJob results vs time\n",
    "Next we will show how the objective metric changes over time, as the tuning job progresses. For Bayesian strategy, you should expect to see a general trend towards better results, but this progress will not be steady as the algorithm needs to balance exploration of new areas of parameter space against exploitation of known good areas. This can give you a sense of whether or not the number of training jobs is sufficient for the complexity of your search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1489\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1489\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1489\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1489' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1489\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1489\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1489\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1489' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1489\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"819b2b14-f40e-46c3-b4d0-3b80319a2fbb\" data-root-id=\"1491\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"483a2644-2803-4c44-bdcb-eeeb558fa227\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1500\",\"type\":\"DatetimeAxis\"}],\"left\":[{\"id\":\"1505\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":900,\"renderers\":[{\"id\":\"1500\",\"type\":\"DatetimeAxis\"},{\"id\":\"1504\",\"type\":\"Grid\"},{\"id\":\"1505\",\"type\":\"LinearAxis\"},{\"id\":\"1509\",\"type\":\"Grid\"},{\"id\":\"1530\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1573\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1517\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1492\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1496\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1494\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1498\",\"type\":\"LinearScale\"}},\"id\":\"1491\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"base\":60,\"mantissas\":[1,2,5,10,15,20,30],\"max_interval\":1800000.0,\"min_interval\":1000.0,\"num_minor_ticks\":0},\"id\":\"1580\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"base\":24,\"mantissas\":[1,2,4,6,8,12],\"max_interval\":43200000.0,\"min_interval\":3600000.0,\"num_minor_ticks\":0},\"id\":\"1581\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"1577\",\"type\":\"DatetimeTickFormatter\"},\"plot\":{\"id\":\"1491\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1501\",\"type\":\"DatetimeTicker\"}},\"id\":\"1500\",\"type\":\"DatetimeAxis\"},{\"attributes\":{\"plot\":{\"id\":\"1491\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1501\",\"type\":\"DatetimeTicker\"}},\"id\":\"1504\",\"type\":\"Grid\"},{\"attributes\":{\"days\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]},\"id\":\"1582\",\"type\":\"DaysTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"1575\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1491\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1506\",\"type\":\"BasicTicker\"}},\"id\":\"1505\",\"type\":\"LinearAxis\"},{\"attributes\":{\"days\":[1,4,7,10,13,16,19,22,25,28]},\"id\":\"1583\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1498\",\"type\":\"LinearScale\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1491\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1506\",\"type\":\"BasicTicker\"}},\"id\":\"1509\",\"type\":\"Grid\"},{\"attributes\":{\"days\":[1,8,15,22]},\"id\":\"1584\",\"type\":\"DaysTicker\"},{\"attributes\":{\"num_minor_ticks\":5,\"tickers\":[{\"id\":\"1579\",\"type\":\"AdaptiveTicker\"},{\"id\":\"1580\",\"type\":\"AdaptiveTicker\"},{\"id\":\"1581\",\"type\":\"AdaptiveTicker\"},{\"id\":\"1582\",\"type\":\"DaysTicker\"},{\"id\":\"1583\",\"type\":\"DaysTicker\"},{\"id\":\"1584\",\"type\":\"DaysTicker\"},{\"id\":\"1585\",\"type\":\"DaysTicker\"},{\"id\":\"1586\",\"type\":\"MonthsTicker\"},{\"id\":\"1587\",\"type\":\"MonthsTicker\"},{\"id\":\"1588\",\"type\":\"MonthsTicker\"},{\"id\":\"1589\",\"type\":\"MonthsTicker\"},{\"id\":\"1590\",\"type\":\"YearsTicker\"}]},\"id\":\"1501\",\"type\":\"DatetimeTicker\"},{\"attributes\":{\"days\":[1,15]},\"id\":\"1585\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1511\",\"type\":\"CrosshairTool\"},{\"attributes\":{\"months\":[0,1,2,3,4,5,6,7,8,9,10,11]},\"id\":\"1586\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1506\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1512\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1492\",\"type\":\"DataRange1d\"},{\"attributes\":{\"months\":[0,2,4,6,8,10]},\"id\":\"1587\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1513\",\"type\":\"ZoomInTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"TrainingStartTime\"},\"y\":{\"field\":\"FinalObjectiveValue\"}},\"id\":\"1529\",\"type\":\"Circle\"},{\"attributes\":{\"months\":[0,4,8]},\"id\":\"1588\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1514\",\"type\":\"ZoomOutTool\"},{\"attributes\":{},\"id\":\"1496\",\"type\":\"LinearScale\"},{\"attributes\":{\"months\":[0,6]},\"id\":\"1589\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"callback\":null},\"id\":\"1494\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1515\",\"type\":\"UndoTool\"},{\"attributes\":{},\"id\":\"1590\",\"type\":\"YearsTicker\"},{\"attributes\":{},\"id\":\"1516\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1591\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1510\",\"type\":\"PanTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1490\",\"type\":\"HoverTool\"},{\"id\":\"1510\",\"type\":\"PanTool\"},{\"id\":\"1511\",\"type\":\"CrosshairTool\"},{\"id\":\"1512\",\"type\":\"WheelZoomTool\"},{\"id\":\"1513\",\"type\":\"ZoomInTool\"},{\"id\":\"1514\",\"type\":\"ZoomOutTool\"},{\"id\":\"1515\",\"type\":\"UndoTool\"},{\"id\":\"1516\",\"type\":\"ResetTool\"}]},\"id\":\"1517\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1592\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"FinalObjectiveValue\",\"@FinalObjectiveValue\"],[\"TrainingJobName\",\"@TrainingJobName\"],[\"mini_batch_size\",\"@{mini_batch_size}\"],[\"learning_rate\",\"@{learning_rate}\"],[\"optimizer\",\"@{optimizer}\"]]},\"id\":\"1490\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1577\",\"type\":\"DatetimeTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"TrainingStartTime\"},\"y\":{\"field\":\"FinalObjectiveValue\"}},\"id\":\"1528\",\"type\":\"Circle\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"1573\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"1526\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1528\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1529\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"1531\",\"type\":\"CDSView\"}},\"id\":\"1530\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"mantissas\":[1,2,5],\"max_interval\":500.0,\"num_minor_ticks\":0},\"id\":\"1579\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{},\"id\":\"1575\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"data\":{\"FinalObjectiveValue\":{\"__ndarray__\":\"AAAAQPjQ7T8AAAAgQ5ztPwAAAGBDjO0/AAAAwNiJ7T8AAABAcw/tPwAAAED8/Ow/AAAAwMzM7D8AAABAYbTsPwAAAKAopew/AAAAgP+f7D8AAABgx3HsPwAAAMDMTOw/AAAA4J7n6z8AAABgZubrPwAAAGAxv+s/AAAAQFvc5D8AAACAarzkPwAAAEAzs+Q/AAAA4FsR5D8AAABA/S68Pw==\",\"dtype\":\"float64\",\"shape\":[20]},\"TrainingElapsedTimeSeconds\":{\"__ndarray__\":\"AAAAAABAWEAAAAAAAEBZQAAAAAAAwFpAAAAAAAAAWUAAAAAAAABYQAAAAAAAwFpAAAAAAACAWkAAAAAAAEBZQAAAAAAAAFtAAAAAAAAAWUAAAAAAAABbQAAAAAAAQFhAAAAAAAAAWkAAAAAAAEBXQAAAAAAAAFlAAAAAAABAV0AAAAAAAEBYQAAAAAAAwFtAAAAAAACAW0AAAAAAAEBWQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"TrainingEndTime\":{\"__ndarray__\":\"AACwELcbd0IAgNlStxt3QgCA/c+2G3dCAAAnErcbd0IAAJKStxt3QgAADFW3G3dCAACcxrUbd0IAAA3JtRt3QgAAU9u3G3dCAADfF7gbd0IAAI0Htht3QgAAB0e2G3dCAIAkDrYbd0IAgCSLtht3QgAAANS3G3dCAABmTLYbd0IAAOuYtxt3QgAA3xe4G3dCAABFkLYbd0IAgGjTtht3Qg==\",\"dtype\":\"float64\",\"shape\":[20]},\"TrainingJobName\":[\"image-classification1-011-99979921\",\"image-classification1-014-bec1fc44\",\"image-classification1-010-9f96cc41\",\"image-classification1-012-f72a40c4\",\"image-classification1-016-0043c0fb\",\"image-classification1-013-5c4b1361\",\"image-classification1-001-d08b859b\",\"image-classification1-002-21b58d0c\",\"image-classification1-018-5e2e2bac\",\"image-classification1-020-759928c0\",\"image-classification1-004-a2aab91b\",\"image-classification1-005-a8999268\",\"image-classification1-003-3769554a\",\"image-classification1-007-74edc825\",\"image-classification1-017-1660cab5\",\"image-classification1-006-6b6c6e03\",\"image-classification1-015-e8f138c7\",\"image-classification1-019-64653c51\",\"image-classification1-008-2cc90ee6\",\"image-classification1-009-8c50a25d\"],\"TrainingJobStatus\":[\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Stopped\",\"Completed\",\"Completed\",\"Stopped\"],\"TrainingStartTime\":{\"__ndarray__\":\"AIAB+bYbd0IAADE6txt3QgAA3rW2G3dCAAC9+bYbd0IAACJ7txt3QgCA7Dq3G3dCAAC7rLUbd0IAgGSwtRt3QgAA9cC3G3dCAAB1/7cbd0IAAC/ttRt3QgCAWC+2G3dCAIDA9LUbd0IAAHB0tht3QgAAlru3G3dCAICxNbYbd0IAgDyBtxt3QgCAxfy3G3dCAABqdbYbd0IAAK69tht3Qg==\",\"dtype\":\"float64\",\"shape\":[20]},\"index\":[9,6,10,8,4,7,19,18,2,0,16,15,17,13,3,14,5,1,12,11],\"learning_rate\":{\"__ndarray__\":\"MMcuW/hslz82kQ8LXzsEPw949pUz+WU/xpSc1JDCJD9HArPLVBVkP97F/kP4NQg/aF34ceiUMj/ZSIPaBG+rP4R9U3Ggbzk/CYXxke/SAD+SasTjecQyPwSKwRhhYfI+mxyr4MF+Hj/vgZD3NQ4dP6bqADjQ3qk/RywqOt5Z0D+Wc/xv6FSEP49M7V4OC9k/7LVsQjQ62z9l5LozAu3vPg==\",\"dtype\":\"float64\",\"shape\":[20]},\"mini_batch_size\":{\"__ndarray__\":\"AAAAAADAX0AAAAAAAEBTQAAAAAAAwFVAAAAAAADAVkAAAAAAAMBfQAAAAAAAAFFAAAAAAAAAUEAAAAAAAMBeQAAAAAAAAF9AAAAAAAAAYEAAAAAAAIBWQAAAAAAAAFBAAAAAAAAAVUAAAAAAAABQQAAAAAAAwFNAAAAAAADAU0AAAAAAAEBfQAAAAAAAAFBAAAAAAACAXUAAAAAAAEBbQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"optimizer\":[\"nag\",\"rmsprop\",\"nag\",\"rmsprop\",\"sgd\",\"rmsprop\",\"sgd\",\"nag\",\"nag\",\"rmsprop\",\"sgd\",\"rmsprop\",\"sgd\",\"nag\",\"sgd\",\"adam\",\"adam\",\"nag\",\"rmsprop\",\"nag\"]},\"selected\":{\"id\":\"1592\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1591\",\"type\":\"UnionRenderers\"}},\"id\":\"1526\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"1526\",\"type\":\"ColumnDataSource\"}},\"id\":\"1531\",\"type\":\"CDSView\"}],\"root_ids\":[\"1491\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.4\"}};\n",
       "  var render_items = [{\"docid\":\"483a2644-2803-4c44-bdcb-eeeb558fa227\",\"roots\":{\"1491\":\"819b2b14-f40e-46c3-b4d0-3b80319a2fbb\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1491"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "class HoverHelper():\n",
    "\n",
    "    def __init__(self, tuning_analytics):\n",
    "        self.tuner = tuning_analytics\n",
    "\n",
    "    def hovertool(self):\n",
    "        tooltips = [\n",
    "            (\"FinalObjectiveValue\", \"@FinalObjectiveValue\"),\n",
    "            (\"TrainingJobName\", \"@TrainingJobName\"),\n",
    "        ]\n",
    "        for k in self.tuner.tuning_ranges.keys():\n",
    "            tooltips.append( (k, \"@{%s}\" % k) )\n",
    "\n",
    "        ht = HoverTool(tooltips=tooltips)\n",
    "        return ht\n",
    "\n",
    "    def tools(self, standard_tools='pan,crosshair,wheel_zoom,zoom_in,zoom_out,undo,reset'):\n",
    "        return [self.hovertool(), standard_tools]\n",
    "\n",
    "hover = HoverHelper(tuner)\n",
    "\n",
    "p = figure(plot_width=900, plot_height=400, tools=hover.tools(), x_axis_type='datetime')\n",
    "p.circle(source=df, x='TrainingStartTime', y='FinalObjectiveValue')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the correlation between objective metric and individual hyperparameters¶\n",
    "Now you have finished a tuning job, you may want to know the correlation between your objective metric and individual hyperparameters you've selected to tune. Having that insight will help you decide whether it makes sense to adjust search ranges for certain hyperparameters and start another tuning job. For example, if you see a positive trend between objective metric and a numerical hyperparameter, you probably want to set a higher tuning range for that hyperparameter in your next tuning job.\n",
    "\n",
    "The following cell draws a graph for each hyperparameter to show its correlation with your objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"9eb0c755-2090-4e9d-b14e-0e6cb3983394\" data-root-id=\"2096\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"f12c7c8c-616f-4b95-af4b-e6db9bf0f5ff\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"2055\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"2096\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"2061\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"2080\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"2075\",\"type\":\"CrosshairTool\"},{\"attributes\":{},\"id\":\"2066\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"2077\",\"type\":\"ZoomInTool\"},{\"attributes\":{\"plot\":null,\"text\":\"Objective vs optimizer\"},\"id\":\"2054\",\"type\":\"Title\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"2055\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2070\",\"type\":\"BasicTicker\"}},\"id\":\"2073\",\"type\":\"Grid\"},{\"attributes\":{\"below\":[{\"id\":\"2065\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"2069\",\"type\":\"LinearAxis\"}],\"plot_height\":500,\"plot_width\":500,\"renderers\":[{\"id\":\"2065\",\"type\":\"CategoricalAxis\"},{\"id\":\"2068\",\"type\":\"Grid\"},{\"id\":\"2069\",\"type\":\"LinearAxis\"},{\"id\":\"2073\",\"type\":\"Grid\"},{\"id\":\"2094\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"2054\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"2081\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"2057\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"2061\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"2059\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"2063\",\"type\":\"LinearScale\"}},\"id\":\"2055\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"2079\",\"type\":\"UndoTool\"},{\"attributes\":{},\"id\":\"2078\",\"type\":\"ZoomOutTool\"},{\"attributes\":{},\"id\":\"2076\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"2180\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null},\"id\":\"2059\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data_source\":{\"id\":\"2090\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"2092\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2093\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"2095\",\"type\":\"CDSView\"}},\"id\":\"2094\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"optimizer\"},\"y\":{\"field\":\"FinalObjectiveValue\"}},\"id\":\"2092\",\"type\":\"Circle\"},{\"attributes\":{\"axis_label\":\"optimizer\",\"formatter\":{\"id\":\"2182\",\"type\":\"CategoricalTickFormatter\"},\"plot\":{\"id\":\"2055\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2066\",\"type\":\"CategoricalTicker\"}},\"id\":\"2065\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"2184\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"2074\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"2182\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"2053\",\"type\":\"HoverTool\"},{\"id\":\"2074\",\"type\":\"PanTool\"},{\"id\":\"2075\",\"type\":\"CrosshairTool\"},{\"id\":\"2076\",\"type\":\"WheelZoomTool\"},{\"id\":\"2077\",\"type\":\"ZoomInTool\"},{\"id\":\"2078\",\"type\":\"ZoomOutTool\"},{\"id\":\"2079\",\"type\":\"UndoTool\"},{\"id\":\"2080\",\"type\":\"ResetTool\"}]},\"id\":\"2081\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"2070\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"2185\",\"type\":\"Selection\"},{\"attributes\":{\"plot\":{\"id\":\"2055\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2066\",\"type\":\"CategoricalTicker\"}},\"id\":\"2068\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"data\":{\"FinalObjectiveValue\":{\"__ndarray__\":\"AAAAQPjQ7T8AAAAgQ5ztPwAAAGBDjO0/AAAAwNiJ7T8AAABAcw/tPwAAAED8/Ow/AAAAwMzM7D8AAABAYbTsPwAAAKAopew/AAAAgP+f7D8AAABgx3HsPwAAAMDMTOw/AAAA4J7n6z8AAABgZubrPwAAAGAxv+s/AAAAQFvc5D8AAACAarzkPwAAAEAzs+Q/AAAA4FsR5D8AAABA/S68Pw==\",\"dtype\":\"float64\",\"shape\":[20]},\"TrainingElapsedTimeSeconds\":{\"__ndarray__\":\"AAAAAABAWEAAAAAAAEBZQAAAAAAAwFpAAAAAAAAAWUAAAAAAAABYQAAAAAAAwFpAAAAAAACAWkAAAAAAAEBZQAAAAAAAAFtAAAAAAAAAWUAAAAAAAABbQAAAAAAAQFhAAAAAAAAAWkAAAAAAAEBXQAAAAAAAAFlAAAAAAABAV0AAAAAAAEBYQAAAAAAAwFtAAAAAAACAW0AAAAAAAEBWQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"TrainingEndTime\":{\"__ndarray__\":\"AACwELcbd0IAgNlStxt3QgCA/c+2G3dCAAAnErcbd0IAAJKStxt3QgAADFW3G3dCAACcxrUbd0IAAA3JtRt3QgAAU9u3G3dCAADfF7gbd0IAAI0Htht3QgAAB0e2G3dCAIAkDrYbd0IAgCSLtht3QgAAANS3G3dCAABmTLYbd0IAAOuYtxt3QgAA3xe4G3dCAABFkLYbd0IAgGjTtht3Qg==\",\"dtype\":\"float64\",\"shape\":[20]},\"TrainingJobName\":[\"image-classification1-011-99979921\",\"image-classification1-014-bec1fc44\",\"image-classification1-010-9f96cc41\",\"image-classification1-012-f72a40c4\",\"image-classification1-016-0043c0fb\",\"image-classification1-013-5c4b1361\",\"image-classification1-001-d08b859b\",\"image-classification1-002-21b58d0c\",\"image-classification1-018-5e2e2bac\",\"image-classification1-020-759928c0\",\"image-classification1-004-a2aab91b\",\"image-classification1-005-a8999268\",\"image-classification1-003-3769554a\",\"image-classification1-007-74edc825\",\"image-classification1-017-1660cab5\",\"image-classification1-006-6b6c6e03\",\"image-classification1-015-e8f138c7\",\"image-classification1-019-64653c51\",\"image-classification1-008-2cc90ee6\",\"image-classification1-009-8c50a25d\"],\"TrainingJobStatus\":[\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Completed\",\"Stopped\",\"Completed\",\"Completed\",\"Stopped\"],\"TrainingStartTime\":{\"__ndarray__\":\"AIAB+bYbd0IAADE6txt3QgAA3rW2G3dCAAC9+bYbd0IAACJ7txt3QgCA7Dq3G3dCAAC7rLUbd0IAgGSwtRt3QgAA9cC3G3dCAAB1/7cbd0IAAC/ttRt3QgCAWC+2G3dCAIDA9LUbd0IAAHB0tht3QgAAlru3G3dCAICxNbYbd0IAgDyBtxt3QgCAxfy3G3dCAABqdbYbd0IAAK69tht3Qg==\",\"dtype\":\"float64\",\"shape\":[20]},\"index\":[9,6,10,8,4,7,19,18,2,0,16,15,17,13,3,14,5,1,12,11],\"learning_rate\":{\"__ndarray__\":\"MMcuW/hslz82kQ8LXzsEPw949pUz+WU/xpSc1JDCJD9HArPLVBVkP97F/kP4NQg/aF34ceiUMj/ZSIPaBG+rP4R9U3Ggbzk/CYXxke/SAD+SasTjecQyPwSKwRhhYfI+mxyr4MF+Hj/vgZD3NQ4dP6bqADjQ3qk/RywqOt5Z0D+Wc/xv6FSEP49M7V4OC9k/7LVsQjQ62z9l5LozAu3vPg==\",\"dtype\":\"float64\",\"shape\":[20]},\"mini_batch_size\":{\"__ndarray__\":\"AAAAAADAX0AAAAAAAEBTQAAAAAAAwFVAAAAAAADAVkAAAAAAAMBfQAAAAAAAAFFAAAAAAAAAUEAAAAAAAMBeQAAAAAAAAF9AAAAAAAAAYEAAAAAAAIBWQAAAAAAAAFBAAAAAAAAAVUAAAAAAAABQQAAAAAAAwFNAAAAAAADAU0AAAAAAAEBfQAAAAAAAAFBAAAAAAACAXUAAAAAAAEBbQA==\",\"dtype\":\"float64\",\"shape\":[20]},\"optimizer\":[\"nag\",\"rmsprop\",\"nag\",\"rmsprop\",\"sgd\",\"rmsprop\",\"sgd\",\"nag\",\"nag\",\"rmsprop\",\"sgd\",\"rmsprop\",\"sgd\",\"nag\",\"sgd\",\"adam\",\"adam\",\"nag\",\"rmsprop\",\"nag\"]},\"selected\":{\"id\":\"2185\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"2184\",\"type\":\"UnionRenderers\"}},\"id\":\"2090\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis_label\":\"validation:accuracy\",\"formatter\":{\"id\":\"2180\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"2055\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2070\",\"type\":\"BasicTicker\"}},\"id\":\"2069\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"factors\":[\"sgd\",\"adam\",\"rmsprop\",\"nag\"]},\"id\":\"2057\",\"type\":\"FactorRange\"},{\"attributes\":{\"source\":{\"id\":\"2090\",\"type\":\"ColumnDataSource\"}},\"id\":\"2095\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"optimizer\"},\"y\":{\"field\":\"FinalObjectiveValue\"}},\"id\":\"2093\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"FinalObjectiveValue\",\"@FinalObjectiveValue\"],[\"TrainingJobName\",\"@TrainingJobName\"],[\"mini_batch_size\",\"@{mini_batch_size}\"],[\"learning_rate\",\"@{learning_rate}\"],[\"optimizer\",\"@{optimizer}\"]]},\"id\":\"2053\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"2063\",\"type\":\"LinearScale\"}],\"root_ids\":[\"2096\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.4\"}};\n",
       "  var render_items = [{\"docid\":\"f12c7c8c-616f-4b95-af4b-e6db9bf0f5ff\",\"roots\":{\"2096\":\"9eb0c755-2090-4e9d-b14e-0e6cb3983394\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "2096"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranges = tuner.tuning_ranges\n",
    "figures = []\n",
    "for hp_name, hp_range in ranges.items():\n",
    "    if(hp_name == 'optimizer'):\n",
    "        categorical_args = {}\n",
    "        #print(hp_name)\n",
    "        if hp_range.get('Values'):\n",
    "            # This is marked as categorical.  Check if all options are actually numbers.\n",
    "            def is_num(x):\n",
    "                try:\n",
    "                    float(x)\n",
    "                    return 1\n",
    "                except:\n",
    "                    return 0           \n",
    "            vals = hp_range['Values']\n",
    "            if sum([is_num(x) for x in vals]) == len(vals):\n",
    "                # Bokeh has issues plotting a \"categorical\" range that's actually numeric, so plot as numeric\n",
    "                print(\"Hyperparameter %s is tuned as categorical, but all values are numeric\" % hp_name)\n",
    "            else:\n",
    "                # Set up extra options for plotting categoricals.  A bit tricky when they're actually numbers.\n",
    "                categorical_args['x_range'] = vals\n",
    "\n",
    "        # Now plot it\n",
    "        p = figure(plot_width=500, plot_height=500, \n",
    "                   title=\"Objective vs %s\" % hp_name,\n",
    "                   tools=hover.tools(),\n",
    "                   x_axis_label=hp_name, y_axis_label=objective_name,\n",
    "                   **categorical_args)\n",
    "\n",
    "        p.circle(source=df, x=hp_name, y='FinalObjectiveValue')\n",
    "        figures.append(p)\n",
    "show(bokeh.layouts.Column(*figures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class of the image. You can deploy the created model by using the deploy method in the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ic_classifier = model.deploy(initial_instance_count = 1,\n",
    "                                          instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "import cv2\n",
    "file_name = 'image/865.jpeg'\n",
    "im = cv2.imread(file_name)\n",
    "print(im.shape)\n",
    "im = cv2.resize(im, (600, 400))\n",
    "cv2.imwrite('test.jpeg', im)\n",
    "\n",
    "# display test image\n",
    "from IPython.display import Image, display\n",
    "img = Image(file_name) \n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluate the image through the network for inference. The network outputs class probabilities for all the classes. As can be seen from this example, the network output is pretty good even with training for only 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown:0.097766 head_lamp:0.902116 door_scratch:0.000008 glass_shatter:0.000012 tail_lamp:0.000009 bumper_dent:0.000006 door_dent:0.000072 bumper_scratch:0.000011 "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(file_name, 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "ic_classifier.content_type = 'application/x-image'\n",
    "results = ic_classifier.predict(b)\n",
    "prob = json.loads(results)\n",
    "#classes = ['Person', 'Bicycle', 'Car', 'Motorcycle', 'Airplane']\n",
    "for idx, val in enumerate(cats):\n",
    "    print('%s:%f '%(cats[idx], prob[idx]), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can use the following command to delete the endpoint. The endpoint that is created above is persistent and would consume resources till it is deleted. It is good to delete the endpoint when it is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
